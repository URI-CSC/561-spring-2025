{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 (due Mar 5th at 11:59pm)\n",
    "\n",
    "In this assignment you will experiment with different flavors of gradient descent using PyTorch.  You can read the code shared via Ed or the code available at this [Colab Notebook](https://colab.research.google.com/drive/1_X-eIixlSeirFcO8KZmIDsbV89k-DfCy).\n",
    "\n",
    "The assignment is worth a total of **100 points**.  Students shall submit a copy of this notebook (`.ipynb`) via [Gradescope](https://gradescope.com), including **well-documented answers** (commented code and comments on the usage of GenAI tools if any).\n",
    "\n",
    "We expect students run the assignment on Google Colab notebooks with a **GPU runtime**. You can go to *Edit/Notebook settings* and change the *hardware accelerator* to GPU.\n",
    "\n",
    ">  **Important**: Make sure all cells are executed before saving/downloading a copy of the notebook you will submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define a Neural Network (10 pt)\n",
    "\n",
    "Here you will create a Pytorch class that defines a simple neural network.  The network will have as many layers as you want and as many neurons as you want.  The activation function will be ReLU for all hidden layers, and it can be linear for the output layer (as long as the selected loss function takes logits as input -- read the docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Define a class named Network that implements a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset (15 pt)\n",
    "\n",
    "Here you will define a function that loads the `FashionMNIST` dataset (read the related documentation to understand all the details about the data). \n",
    "\n",
    "The function should have the following signature:\n",
    "\n",
    "```python\n",
    "def load_dataset(batch_size)\n",
    "```\n",
    "\n",
    "- `batch_size` is the size of the mini-batches\n",
    "\n",
    "The function should download/load the FashionMNIST dataset, preprocess it, and return the training, validation, and test data loaders.  Preprocessing involves transforming the data into a Pytorch tensor. \n",
    "\n",
    "The training data loader should only include a *subset of 5000 datapoints* drawn randomly from the *train* partition of the FashionMNIST dataset (there are 60000 samples in this partition).  This function should also split the *test* partition of the FashionMNIST dataset evenly (there are 10000 samples in this partition) into validation (50%) and test (50%).\n",
    "\n",
    "The training data loader should shuffle the data, while the validation and test data loaders should not shuffle the data.  \n",
    "\n",
    "Optionally, inside this function you may want to divide all values by 255 to normalize the data as the FashionMNIST images range from 0 to 255.\n",
    "\n",
    "At the end of the function include a `print` statement that reports the shapes of the input and target tensors for each data loader.  We expect to see 10000, 5000, and 5000 samples on the train, valid, and test data loaders respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Define a function to load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Gradient Descent (20 pt)\n",
    "\n",
    "Here you will train a neural network using batch gradient descent.  You can use the `load_dataset` function to get the data loaders, unsing `batch_size=5000`, to get the full \"training set\". \n",
    "\n",
    "Make sure that every gradient update uses the full training set.  Feel free to use the `torch.optim` package to help you with the optimization.  For example, you can use the `Adam` optimizer with the default parameters.\n",
    "\n",
    "Print the average loss and classification accuracy for the training and validation sets at every epoch.  You can use the a cross-entropy loss to compute the loss.  You can run this step for a convenient number of epochs, maximizing performance and reducing training time.\n",
    "\n",
    "After the model is trained, print the loss and classification accuracy of the train model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Code for the task above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic Gradient Descent (20 pt)\n",
    "\n",
    "Here you will train a neural network using batch gradient descent.  You can use the `load_dataset` function to get the data loaders, unsing `batch_size=1`, to get the behavior of SGD. \n",
    "\n",
    "Make sure that every gradient update uses the full training set.  Feel free to use the `torch.optim` package to help you with the optimization.  For example, you can use the `Adam` optimizer with the default parameters.\n",
    "\n",
    "Print the average loss and classification accuracy for the training and validation sets at every epoch.  You can use the a cross-entropy loss to compute the loss.  You can run this step for a convenient number of epochs, maximizing performance and reducing training time.\n",
    "\n",
    "After the model is trained, print the loss and classification accuracy of the train model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Code for the task above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mini-Batch Gradient Descent (20 pt)\n",
    "\n",
    "Here you will train a neural network using batch gradient descent.  You can use the `load_dataset` function to get the data loaders, unsing `batch_size=<pick the best value>` maximizing performance and reducing training time. \n",
    "\n",
    "Make sure that every gradient update uses the full training set.  Feel free to use the `torch.optim` package to help you with the optimization.  For example, you can use the `Adam` optimizer with the default parameters.\n",
    "\n",
    "Print the average loss and classification accuracy for the training and validation sets at every epoch.  You can use the a cross-entropy loss to compute the loss.  You can run this step for a convenient number of epochs, maximizing performance and reducing training time.\n",
    "\n",
    "After the model is trained, print the loss and classification accuracy of the train model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Code for the task above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis (15 pt)\n",
    "\n",
    "Provide a comprehensive analysis of the results obtained in the experiments above.  Discuss the impact on performance and training time of using different approaches, full batch, SGD, and mini-batch.  \n",
    "\n",
    "Finally, indicate any final thoughts on the experiments, what are your preferred configurations, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
